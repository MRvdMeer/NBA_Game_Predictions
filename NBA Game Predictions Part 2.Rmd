---
title: "NBA Game Predictions Part 2"
output: html_notebook
---

$$
y_i \sim \mathrm{N}(\mu_i, \sigma) \\
\mu_i = \alpha_{k[i]} + \beta_{k[i]} * x_i \\
\left[\begin{array}{cc}
\alpha_k\\
\beta_k
\end{array}\right] 
\sim \mathrm{N}_2(\boldsymbol{\mu, {S}}) \\
\alpha \sim \mathrm{N}(0, 10) \\
\beta \sim \mathrm{N}(0, 1) \\
\boldsymbol{S} = 
\left(\begin{array}{cc}
\sigma_\alpha & 0 \\
0 & \sigma_\beta
\end{array}\right) C 
\left(\begin{array}{cc}
\sigma_\alpha & 0 \\
0 & \sigma_\beta
\end{array}\right) \\
(\sigma_\alpha, \sigma_\beta) \sim \mathrm{HalfCauchy}(0, 1) \\
C \sim \mathrm{LKJcorr}(2)
$$


# Introduction

This is part 2 in our series on NBA game predictions. In part 1 we built some models. In this part we'll build different models.

In this part we will focus on modeling the difference in score between the two teams for a single game.


# Initial setup

```{r load packages and further setup, message=FALSE, include=FALSE}
library(tidyverse)
library(rstan)
library(bayesplot)

theme_set(bayesplot::theme_default())

options(mc.cores = parallel::detectCores())

source("NBA_utilities.R")

set.seed(12)
```

```{r load data and some data manipulation, include = FALSE}
game_data <- read_csv("NBA Game Data 20162017.csv")
game_data <- game_data %>% 
                select(date = Date, away_team = "Visitor/Neutral", home_team = "Home/Neutral", pts_away = PTS_A, pts_home = PTS_H, overtime = OT) %>%
                mutate(overtime = ifelse (is.na(overtime), 0L, 
                                          ifelse(stringr::str_length(overtime) == 2, 1L, as.integer(substring(overtime, 1, 1)))),
                       score_diff = ifelse(overtime == 0, pts_away - pts_home, 0L))

# create a single identifier used for both away and home teams
unique_teams <- sort(unique(c(game_data$away_team, game_data$home_team)))
team_mapping_table <- tibble(team_name = unique_teams, team_id = 1:length(unique_teams))

team_id <- map_int(c(game_data$away_team, game_data$home_team), lookup_team_id, mapping_table = team_mapping_table)
N_games <- nrow(game_data)
N_teams = length(team_mapping_table$team_id)
game_data <- game_data %>% mutate(away_team_id = team_id[1:N_games], home_team_id = team_id[(N_games + 1):(2 * N_games)])
N_games_per_team <- 82
n_sims <- 4000
```


# Some quick data exploration

```{r quick summary of the data}
paste("Average score difference:", round(mean(game_data$score_diff), 2))
paste("Standard deviation of score difference:", round(sd(game_data$score_diff), 2))
paste("Number of overtime games:", round(sum(game_data$score_diff == 0), 2))

paste("Average points scored by away team:", round(mean(game_data$pts_away), 2))
paste("Variance of away points:", round(var(game_data$pts_away), 2))
paste("Average points scored by home team:", round(mean(game_data$pts_home), 2))
paste("Variance of home points:", round(var(game_data$pts_home), 2))

summary(game_data)
```

Here we see both the home and away points scored are a bit over-dispersed when compared to a Poisson distribution.

```{r some quick plots}
ggplot(data = game_data, mapping = aes(x = score_diff)) + geom_density(color = "blue")

game_data %>%
  gather(key = "away_home", value = "points", pts_away, pts_home) %>%
  ggplot(aes(x = points, col = away_home)) + geom_density() + scale_color_manual(values = c("orange", "darkblue"))

ggplot(data = game_data, mapping = aes(x = score_diff)) + geom_histogram(binwidth = 1, fill = "blue", color = "black")

game_data %>%
  gather(key = "away_home", value = "points", pts_away, pts_home) %>%
  ggplot(aes(x = points, col = away_home, fill = away_home)) + geom_histogram(binwidth = 1) + scale_fill_manual(values = c("orange", "darkblue")) + facet_wrap("away_home")

```


It appears the observed score difference seems to be bimodal, and there appears to be some home-court advantage (more negative score differences than positive). We will use this in the model later.
Also, the distribution of points scored seems relatively bell-shaped, with perhaps a bit more mass towards the tails than we would expect from a normal distribution.
Finally, the distribution shows that overtime games are more frequent than might be expected by pure chance.



# Modeling game outcomes with Stan

```{r set up data for Stan}
stan_data <- list(
                    N_games = N_games,
                    N_teams = N_teams,
                    away_points = game_data$pts_away,
                    home_points = game_data$pts_home,
                    overtime = game_data$overtime,
                    away_team_id = game_data$away_team_id,
                    home_team_id = game_data$home_team_id
                  )
```


## Modeling the season as a whole

First we'll try to fit a model to all games at once. 

### Models based on the Gelman Worldcup example - modeling score difference

First of all, we'll use a model based on [Andrew Gelman's World Cup example](https://andrewgelman.com/2014/07/15/stan-world-cup-update/). Here we model the score difference in a game using a symmetric distribution centered on the skill difference between the two teams.

The model works as follows: each team has a hidden parameter that determines its skill, and for any game, the difference in skill between the two teams determines the expected outcome. For now this is modeled with a normal distribution, although we will look into a Student's t-distribution as well.

```{r compile and fit base Stan model, include = FALSE}
stan_model_bad_comp <- stan_model(file = "Stan_models/NBA_base_model_normal.stan")

stan_fit_bad <- sampling(stan_model_bad_comp, data = stan_data, iter = n_sims)
```

```{r analyze base Stan model}
print(stan_fit_bad, pars = c("team_skill", "stdev"))
```

The base model shows a very bad fit. The reason is that the model is non-identifiable: since we only model the difference in points scored, we can add an arbitrary constant to home skill and away skill for all teams in the dataset and this will lead to the exact same inference. We'll quickly fix this by adding some weak priors.

```{r compile and fit weak prior normal Stan model, include = FALSE}
stan_model_base_comp <- stan_model(file = "Stan_models/NBA_base_model_normal_weak_priors.stan")

stan_fit_base <- sampling(stan_model_base_comp, data = stan_data, iter = n_sims)
```

```{r analyze weak prior normal Stan model}
print(stan_fit_base, pars = c("team_skill", "stdev"))
```

This looks much better - the convergence diagnostics show no problems now. We see the Golden State Warriors are the best team here and the Brooklyn Nets are the worst. This is not a surprise - these were the teams that won the most / least games, respectively. Let's see how well we re-predict the game data.

```{r posterior predictive check base model}
y_rep_base <- as.matrix(stan_fit_base, pars = "score_difference_rep")

ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_base[1:400, ])
```

The replicated datasets seem to re-predict the actual score differences reasonably well, although it seems that the actual data is a bit more skewed to the left (i.e. in favor of the home team). Perhaps the tails also don't work quite well. We'll try to tackle both of these next.

```{r compile and fit t Stan model, include = FALSE}
stan_model_t_comp <- stan_model(file = "Stan_models/NBA_model_t.stan")

stan_fit_t <- sampling(stan_model_t_comp, data = stan_data, iter = n_sims)
```

```{r analyze t Stan model}
print(stan_fit_t, pars = c("team_skill", "scale", "deg_free"))
```

```{r posterior predictive check t model}
y_rep_t <- as.matrix(stan_fit_t, pars = "score_difference_rep")

ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_t[1:400, ])
```

The t-model seems to have fatter tails than necessary for our purposes. For now we'll stick with the normal model. A next potential improvement is to allow for home court advantage.

```{r compile and fit Stan model with home court advantage, include = FALSE}
stan_model_home_comp <- stan_model(file = "Stan_models/NBA_model_normal_home.stan")

stan_fit_home <- sampling(stan_model_home_comp, data = stan_data, iter = n_sims)
```

```{r analyze Stan model with home court advantage}
print(stan_fit_home, pars = c("team_skill", "home_court_advantage", "scale"))
```

It seems home court advantage is worth roughly 3 points in the NBA in this season.

```{r posterior predictive check home court model}
y_rep_home <- as.matrix(stan_fit_home, pars = "score_difference_rep")

ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_home[1:400, ])
```

One more thing we can try is to allow each team to have a different home court advantage. This makes sense because some teams play at a high altitute (like Denver) or might have other factors that make it particularly difficult for visiting teams to play well.

```{r compile and fit normal model with home vector, include = FALSE}
stan_model_hv_comp <- stan_model(file = "Stan_models/NBA_model_normal_home_vector.stan")

stan_fit_hv <- sampling(stan_model_hv_comp, data = stan_data, iter = n_sims)
```

```{r analyze normal model with home vector}
print(stan_fit_hv, pars = c("team_skill", "scale", "mu_home", "sigma_home", "home_court_advantage"))
```

It actually appears there is very little variation in home court advantage for the different teams. In fact, Denver seems to have less home court advantage than most teams!

```{r posterior predictive check normal home vector model}
y_rep_hv <- as.matrix(stan_fit_hv, pars = "score_difference_rep")

ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_hv[1:400, ])
```


### Count models for the individual team scores

So far we have modeled the score difference only, but now we'll try to directly model the points scored by the home and away teams. First we'll try a Poisson model. We've seen earlier that the points scored seem slighly over-dispersed, so we'll keep that in mind here. We model points scored as a Poisson random variable based on the team (unobserved) skill of the away and home teams. This is not perfect: team skill should not only determine points scored, but also make it more difficult for the opposing team to score. We'll revisit this later; for now we can think of team skill as just ability to score points.

```{r compile and fit Stan Poisson model, include = FALSE}
stan_model_poisson_comp <- stan_model(file = "Stan_models/NBA_model_poisson.stan")

stan_fit_poisson <- sampling(stan_model_poisson_comp, data = stan_data, iter = n_sims)
```

```{r analyze Stan Poisson model}
print(stan_fit_poisson, pars = c("team_skill", "home_court_advantage"))
```

```{r posterior predictive check Poisson model}
y_rep_poisson <- as.matrix(stan_fit_poisson, pars = c("away_score_rep", "home_score_rep", "score_difference_rep"))

ppc_dens_overlay(y = game_data$pts_away, yrep = y_rep_poisson[1:400, 1:N_games])
ppc_dens_overlay(y = game_data$pts_home, yrep = y_rep_poisson[1:400, (N_games + 1):(2 * N_games)])
ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_poisson[1:400, (2 * N_games + 1):(3 * N_games)])
```

The Poisson model predicts the points scored quite well, and as a result the difference in points scored too. We'll investigate whether a Negative Binomial model does even better - we noticed some over-dispersion earlier and this might help fix that.

```{r compile and fit Stan Negative Binomial model, include = FALSE}
stan_model_negbin_comp <- stan_model(file = "Stan_models/NBA_model_negbin.stan")

stan_fit_negbin <- sampling(stan_model_negbin_comp, data = stan_data, iter = n_sims)
```

```{r analyze Stan Negative Binomial model}
print(stan_fit_negbin, pars = c("team_skill", "home_court_advantage", "precision"))
```

```{r posterior predictive check Negative Binomial model}
y_rep_negbin <- as.matrix(stan_fit_negbin, pars = c("away_score_rep", "home_score_rep", "score_difference_rep"))

ppc_dens_overlay(y = game_data$pts_away, yrep = y_rep_negbin[1:400, 1:N_games])
ppc_dens_overlay(y = game_data$pts_home, yrep = y_rep_negbin[1:400, (N_games + 1):(2 * N_games)])
ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_negbin[1:400, (2 * N_games + 1):(3 * N_games)])
```

It's difficult to say whether this fit is better than the Poisson model. While the points scored seem to fit even better, the difference in points scored seems to be a worse fit. This model predicts a bigger probability for very extreme outcomes.

Instead of continuing with the Negative Binomial model, we'll try to modify the Poisson model. A reason why we may miss some variation is that different teams might have different levels of home-court advantage. Maybe we just need a home court advantage vector in addition to the skill vector.

Another thing that may be improved is that we don't use the overtime variable anymore - in overtime games the score tends to be higher since there are more minutes in such a game. We might try to model points per minute instead of points per game.

Finally, we might introduce separate parameters for offensive and defensive skill.

```{r compile and fit Stan Poisson Home Vector model, include = FALSE}
stan_model_poisson_hv_comp <- stan_model(file = "Stan_models/NBA_model_poisson_home_vector.stan")

stan_fit_poisson_hv <- sampling(stan_model_poisson_hv_comp, data = stan_data, iter = n_sims, chains = 4,
                                # specifying initial values because they get rejected otherwise
                                # this SHOULD be fixed by modeling on the log-scale
                                # but we're lazy right now and won't change the model.
                                init = list(list(home_court_advantage_raw = rep(0, N_teams),
                                                 mu_home = 1),
                                            list(home_court_advantage_raw = rep(1, N_teams),
                                                 mu_home = 0),
                                            list(home_court_advantage_raw = rep(2, N_teams),
                                                 mu_home = 0),
                                            list(home_court_advantage_raw = rep(3, N_teams),
                                                 mu_home = 0)
                                            )
                                )
```

```{r analyze Stan Poisson Home Vector model}
print(stan_fit_poisson_hv, pars = c("team_skill", "home_court_advantage"))
```

We see quite a different story from earlier - here there seems to be a much clearer variation in home court advantage for different teams. In the worst case this is worth less than two points and in the best case this is worth more than 4.5 points. Maybe the reason this wasn't obvious with the score difference model is that teams may play at a different pace at home or on the road. As a result they may score more points but the opponents as well. We'll keep this in mind for potential model expansion later.

```{r posterior predictive check Poisson Home Vector model}
y_rep_poisson_hv <- as.matrix(stan_fit_poisson_hv, pars = c("away_score_rep", "home_score_rep", "score_difference_rep"))

ppc_dens_overlay(y = game_data$pts_away, yrep = y_rep_poisson_hv[1:400, 1:N_games])
ppc_dens_overlay(y = game_data$pts_home, yrep = y_rep_poisson_hv[1:400, (N_games + 1):(2 * N_games)])
ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_poisson_hv[1:400, (2 * N_games + 1):(3 * N_games)])
```

Now let's also fit a points per minute model, based on the home vector model. In this model the hidden "skill" parameter determines scoring ability per minute.

```{r compile and fit Stan Poisson Minutes model, include = FALSE}
stan_model_pois_min <- stan_model(file = "Stan_models/NBA_model_poisson_minutes.stan")

stan_fit_pois_min <- sampling(stan_model_pois_min, data = stan_data, iter = n_sims, chains = 4, 
                                # specifying initial values because they get rejected otherwise
                                # this SHOULD be fixed by modeling on the log-scale
                                # but we're lazy right now and won't change the model.
                                init = list(list(home_court_advantage_raw = rep(0, N_teams),
                                                 mu_home = 1),
                                            list(home_court_advantage_raw = rep(1, N_teams),
                                                 mu_home = 0),
                                            list(home_court_advantage_raw = rep(2, N_teams),
                                                 mu_home = 0),
                                            list(home_court_advantage_raw = rep(3, N_teams),
                                                 mu_home = 0)
                                            )
                              )
```

```{r analyze Stan Poisson Minutes model}
print(stan_fit_pois_min, pars = c("team_skill", "home_court_advantage"))
```

Convergence diagnostics look fine.

```{r posterior predictive check Poisson Minutes model}
y_rep_pois_min <- as.matrix(stan_fit_pois_min, pars = c("away_score_rep", "home_score_rep", "score_difference_rep"))

ppc_dens_overlay(y = game_data$pts_away, yrep = y_rep_pois_min[1:400, 1:N_games])
ppc_dens_overlay(y = game_data$pts_home, yrep = y_rep_pois_min[1:400, (N_games + 1):(2 * N_games)])
ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_pois_min[1:400, (2 * N_games + 1):(3 * N_games)])
```

Posterior predictive plots also look fine. 

Next, we'll build a model that accounts for offensive and defensive skill. Here we'll model the parameters on the log scale so that the sampler doesn't need arbitrary initial values. This comes with a major disadvantage though; the parameters are now multiplicative and less interpretable.

```{r compile and fit Stan Poisson Offense Defense model, include = FALSE}
stan_model_pois_offdef <- stan_model(file = "Stan_models/NBA_model_poisson_offdef.stan")

stan_fit_pois_offdef <- sampling(stan_model_pois_offdef, data = stan_data, iter = 2 * n_sims, chains = 4,
                                # specifying initial values because they get rejected otherwise
                                # this SHOULD be fixed by modeling on the log-scale
                                # but we're lazy right now and won't change the model.
                                init = list(list(off_skill = rep(10, N_teams)),
                                            list(off_skill = rep(20, N_teams)),
                                            list(off_skill = rep(30, N_teams)),
                                            list(off_skill = rep(40, N_teams))
                                            )
                                )
```

```{r analyze Stan Poisson Offense Defense model}
print(stan_fit_pois_offdef, pars = c("off_skill", "def_skill", "home_court_advantage"))

check_hmc_diagnostics(stan_fit_pois_offdef)
```

Convergence for the skill parameters is good, but effective sample size is a lot smaller. We see home court advantage be about half of what it was in previous models, which makes sense since it is counted double here. Here we can see that Utah is a great defensive team, and Golden State and Houston are great offensive teams. Still, we see many teams defend better than Golden State, so this probably is mainly a function of how fast a team plays. We could potentially still improve this model by adjusting for pace.

```{r posterior predictive check Poisson Offense Defense model}
y_rep_pois_offdef <- as.matrix(stan_fit_pois_offdef, pars = c("away_score_rep", "home_score_rep", "score_difference_rep"))

ppc_dens_overlay(y = game_data$pts_away, yrep = y_rep_pois_offdef[1:400, 1:N_games])
ppc_dens_overlay(y = game_data$pts_home, yrep = y_rep_pois_offdef[1:400, (N_games + 1):(2 * N_games)])
ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_pois_offdef[1:400, (2 * N_games + 1):(3 * N_games)])
```

Predictive checks mostly seem fine, but we still see more density in the middle of the distribution than this model expects.


## Time-series structure

So far we have only fit the model on the entire season at once. Another approach would be to allow the skill of the teams to vary over time. This makes it possible to estimate the form of a team at various moments in the season.


```{r set up data for Stan time series modeling}
stan_data_ts <- list(
                    N_games = N_games,
                    N_teams = length(team_mapping_table$team_id),
                    away_points = game_data$pts_away,
                    home_points = game_data$pts_home,
                    overtime = game_data$overtime,
                    away_team_id = game_data$away_team_id,
                    home_team_id = game_data$home_team_id,
                    N_games_per_team = N_games_per_team
                  )

n_sims_ts <- 10000
```


### Time-series model for score difference

First we'll revisit the simpler normal model for score differences with home court, but now we'll update our estimate of the teams' skill parameters after each game.

```{r compile and fit Stan time-series normal model, include = FALSE}
stan_model_norm_ts_comp <- stan_model(file = "Stan_models/NBA_model_normal_ts.stan")

stan_fit_norm_ts <- sampling(stan_model_norm_ts_comp, data = stan_data_ts, iter = n_sims_ts,
                          control = list(max_treedepth = 15))
```


```{r analyze Stan time-series normal model}
print(stan_fit_norm_ts, pars = c("home_court_advantage", "score_scale", "init_scale", "update_scale"))

check_hmc_diagnostics(stan_fit_norm_ts)
```

```{r further analyze Stan time-series normal model}
# options(max.print = 99999)
print(stan_fit_norm_ts, pars = "team_skill")
# options(max.print = 1000)
```

```{r posterior predictive check time-series normal model}
y_rep_norm_ts <- as.matrix(stan_fit_norm_ts, pars = "score_difference_rep")

ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_norm_ts[1:400, ])
```

The number of effective samples for update_scale is relatively low, but other than that the fit seems to be quite good. Even the team skill parameters (as far as we can see)

Now let's see how team skills evolve over time.


```{r extract skill over time for normal model}
team_skill <- extract(stan_fit_norm_ts, pars = "team_skill", permuted = TRUE)
team_skill <- team_skill[[1]]
mean_skill <- apply(team_skill, c(2, 3), mean) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
median_skill <- apply(team_skill, c(2, 3), median) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
upper_skill <- apply(team_skill, c(2, 3), quantile, probs = c(.96)) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
lower_skill <- apply(team_skill, c(2, 3), quantile, probs = c(.04)) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
```

```{r analyze skill over time for normal model}
mean_skill <- mean_skill %>% gather(key = "game", value = "skill", -team_name) %>%
  mutate(game_number = as.numeric(substring(game, first = 2)))

 ranked_teams <- mean_skill %>% group_by(team_name) %>% 
  summarize(mean_skill = mean(skill),
            median_skill = median(skill),
            max_skill = max(skill),
            min_skill = min(skill)) %>%
  arrange(desc(mean_skill))
 
print(ranked_teams, n = 30, digits = 2)

mean_skill %>% filter(team_name %in% ranked_teams$team_name[1:6]) %>% 
  ggplot(mapping = aes(x = game_number, y = skill, color = team_name)) +
  geom_line() + scale_color_brewer(type = "qual", palette = 3) +
  ggtitle("Top Teams in 2016-2017")

mean_skill %>% filter(team_name %in% ranked_teams$team_name[25:30]) %>% 
  ggplot(mapping = aes(x = game_number, y = skill, color = team_name)) +
  geom_line() + scale_color_brewer(type = "qual", palette = 3) +
  ggtitle("Bottom Teams in 2016-2017")


```


### Time-series model for score

Like before we'll use a Poisson distribution to model points scored by away and home teams directly.

THIS MODEL STILL NEEDS TO BE COMPLETED.


```{r compile and fit Stan time-series Poisson model, include = FALSE}
stan_model_pois_ts_comp <- stan_model(file = "Stan_models/NBA_model_pois_ts.stan")

stan_fit_pois_ts <- sampling(stan_model_pois_ts_comp, data = stan_data_ts, iter = n_sims_ts,
                          control = list(max_treedepth = 15))
```


```{r analyze Stan time-series Poisson model}
print(stan_fit_pois_ts, pars = c("home_court_advantage", "score_scale", "init_scale", "update_scale"))

check_hmc_diagnostics(stan_fit_pois_ts)
```

```{r further analyze Stan time-series Poisson model}
# options(max.print = 99999)
print(stan_fit_pois_ts, pars = "team_skill")
# options(max.print = 1000)
```

```{r posterior predictive check time-series Poisson model}
y_rep_pois_ts <- as.matrix(stan_fit_pois_ts, pars = "score_difference_rep")

ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_pois_ts[1:400, ])
```


```{r extract skill over time for Poisson model}
team_skill <- extract(stan_fit_pois_ts, pars = "team_skill", permuted = TRUE)
team_skill <- team_skill[[1]]
mean_skill <- apply(team_skill, c(2, 3), mean) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
median_skill <- apply(team_skill, c(2, 3), median) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
upper_skill <- apply(team_skill, c(2, 3), quantile, probs = c(.96)) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
lower_skill <- apply(team_skill, c(2, 3), quantile, probs = c(.04)) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
```

```{r analyze skill over time for Poisson model}
mean_skill <- mean_skill %>% gather(key = "game", value = "skill", -team_name) %>%
  mutate(game_number = as.numeric(substring(game, first = 2)))

 ranked_teams <- mean_skill %>% group_by(team_name) %>% 
  summarize(mean_skill = mean(skill),
            median_skill = median(skill),
            max_skill = max(skill),
            min_skill = min(skill)) %>%
  arrange(desc(mean_skill))
 
print(ranked_teams, n = 30, digits = 2)

mean_skill %>% filter(team_name %in% ranked_teams$team_name[1:6]) %>% 
  ggplot(mapping = aes(x = game_number, y = skill, color = team_name)) +
  geom_line() + scale_color_brewer(type = "qual", palette = 3) +
  ggtitle("Top Teams in 2016-2017")

mean_skill %>% filter(team_name %in% ranked_teams$team_name[25:30]) %>% 
  ggplot(mapping = aes(x = game_number, y = skill, color = team_name)) +
  geom_line() + scale_color_brewer(type = "qual", palette = 3) +
  ggtitle("Bottom Teams in 2016-2017")


```




# Predicting new games


So far we have checked the models by seeing how well they re-create the original data through simulations. Another way to check is to fit the models on part of the data and see if you can predict the outcomes of other games. Because of the time-series nature of the data we will only fit the models on sequential data and then predict on newer data.

For now we'll just split the season into a training period and then a prediction period - can we predict the next couple of games after training the model?


```{r additional data manipulation for predictions}
# arbitrary split of the season

N_games_played <- 200 # we'll fit the model on this many games
N_new_games <- N_games - N_games_played
#N_new_games <- 10 # we'll predict this many games after training the model

game_data_played <- game_data[1:N_games_played, ]
game_data_new <- game_data[(N_games_played + 1):(N_games_played + N_new_games) ,]

#game_data_pred <- lookup_amount_games_played(game_data, team_mapping_table, N_games_per_team)

stan_data_pred <- list(
                    N_games = N_games_played,
                    N_new_games = N_new_games,
                    N_teams = N_teams,
                    away_points = game_data_played$pts_away,
                    home_points = game_data_played$pts_home,
                    overtime = game_data_played$overtime,
                    away_team_id = game_data_played$away_team_id,
                    home_team_id = game_data_played$home_team_id,
                    away_id_new = game_data_new$away_team_id,
                    home_id_new = game_data_new$home_team_id
                  )

n_sims_pred <- 4000

```

```{r compile and fit predictive normal model, include = FALSE}
stan_pred_normal_comp <- stan_model(file = "Stan_models/NBA_pred_normal.stan")

stan_pred_normal_fit <- sampling(stan_pred_normal_comp, data = stan_data_pred, iter = n_sims_pred)
```

```{r analyze output of predictive normal model}
print(stan_pred_normal_fit, pars = c("score_difference_pred"))

game_data_new %>% select(date, away_team, home_team, pts_away, pts_home, overtime, score_diff) %>% print(n = 10)

score_diff_predictions <- as.matrix(stan_pred_normal_fit, pars = c("score_difference_pred"))
score_diff_actual <- game_data_new$score_diff

cum_probs <- compare_predictions_to_actual(score_diff_predictions, score_diff_actual)
# print(cum_probs)
quantile(cum_probs, probs = c(0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99))

predicted_outcome <- compute_predicted_outcome(score_diff_predictions)
actual_outcome <- sign(score_diff_actual)

prediction_table <- matrix(c(predicted_outcome, actual_outcome, predicted_outcome == actual_outcome), ncol = 3)
prediction_quality <- mean(abs(predicted_outcome - actual_outcome) / 2) # lower is better

print(prediction_quality)
# print(prediction_table)
```

After training on 100 games and predicting 10 games, we see multiple games where the actual score difference is outside the 95% posterior interval for the predicted score difference. After training on 200 games we still had this problem. We see a couple of extreme outcomes (above 98% cumulative probability). Still, this may be a result of the model used. 

The framework seems to work though, so now let's try to use some more sophisticated models for prediction.

```{r compile and fit predictive normal home model, include = FALSE}
stan_pred_normal_home_comp <- stan_model(file = "Stan_models/NBA_pred_normal_home.stan")

stan_pred_normal_home_fit <- sampling(stan_pred_normal_home_comp, data = stan_data_pred, iter = n_sims_pred)
```

```{r analyze output of predictive normal home model}
print(stan_pred_normal_home_fit, pars = c("score_difference_pred", "home_court_advantage"))

game_data_new %>% select(date, away_team, home_team, pts_away, pts_home, overtime, score_diff) %>% print(n = 10)

score_diff_predictions <- as.matrix(stan_pred_normal_home_fit, pars = c("score_difference_pred"))
score_diff_actual <- game_data_new$score_diff

cum_probs <- compare_predictions_to_actual(score_diff_predictions, score_diff_actual)
# print(cum_probs)
quantile(cum_probs, probs = c(0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99))

predicted_outcome <- compute_predicted_outcome(score_diff_predictions)
actual_outcome <- sign(score_diff_actual)

prediction_table <- matrix(c(predicted_outcome, actual_outcome, predicted_outcome == actual_outcome), ncol = 3)
prediction_quality <- mean(abs(predicted_outcome - actual_outcome) / 2) # lower is better

print(prediction_quality)
# print(prediction_table)
```

We still see quite a few extreme outcomes, so let's try to make predictions using a t model.


```{r compile and fit predictive t home model, include = FALSE}
stan_pred_t_home_comp <- stan_model(file = "Stan_models/NBA_pred_t_home.stan")

stan_pred_t_home_fit <- sampling(stan_pred_t_home_comp, data = stan_data_pred, iter = n_sims_pred)
```

```{r analyze output of predictive t home model}
print(stan_pred_t_home_fit, pars = c("deg_free", "score_difference_pred", "home_court_advantage"))

game_data_new %>% select(date, away_team, home_team, pts_away, pts_home, overtime, score_diff) %>% print(n = 10)

score_diff_predictions <- as.matrix(stan_pred_t_home_fit, pars = c("score_difference_pred"))
score_diff_actual <- game_data_new$score_diff

cum_probs <- compare_predictions_to_actual(score_diff_predictions, score_diff_actual)
# print(cum_probs)
quantile(cum_probs, probs = c(0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99))

predicted_outcome <- compute_predicted_outcome(score_diff_predictions)
actual_outcome <- sign(score_diff_actual)

prediction_table <- matrix(c(predicted_outcome, actual_outcome, predicted_outcome == actual_outcome), ncol = 3)
prediction_quality <- mean(abs(predicted_outcome - actual_outcome) / 2) # lower is better

print(prediction_quality)
# print(prediction_table)
```


The quantiles seem slightly better calibrated, but the win/loss predictions aren't when compared to the normal model with home court advantage. The degrees of freedom parameter is estimated to be quite low; this is evidence against the normal model.







### Count models for the individual team scores

So far we have modeled the score difference only, but now we'll try to directly model the points scored by the home and away teams. First we'll try a Poisson model. We've seen earlier that the points scored seem slighly over-dispersed, so we'll keep that in mind here. We model points scored as a Poisson random variable based on the team (unobserved) skill of the away and home teams. This is not perfect: team skill should not only determine points scored, but also make it more difficult for the opposing team to score. We'll revisit this later; for now we can think of team skill as just ability to score points.

```{r __compile and fit Stan Poisson model, include = FALSE}
stan_model_poisson_comp <- stan_model(file = "Stan_models/NBA_model_poisson.stan")

stan_fit_poisson <- sampling(stan_model_poisson_comp, data = stan_data, iter = n_sims)
```

```{r __analyze Stan Poisson model}
print(stan_fit_poisson, pars = c("team_skill", "home_court_advantage"))
```

```{r __posterior predictive check Poisson model}
y_rep_poisson <- as.matrix(stan_fit_poisson, pars = c("away_score_rep", "home_score_rep", "score_difference_rep"))

ppc_dens_overlay(y = game_data$pts_away, yrep = y_rep_poisson[1:400, 1:N_games])
ppc_dens_overlay(y = game_data$pts_home, yrep = y_rep_poisson[1:400, (N_games + 1):(2 * N_games)])
ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_poisson[1:400, (2 * N_games + 1):(3 * N_games)])
```

The Poisson model predicts the points scored quite well, and as a result the difference in points scored too. We'll investigate whether a Negative Binomial model does even better - we noticed some over-dispersion earlier and this might help fix that.

```{r __compile and fit Stan Negative Binomial model, include = FALSE}
stan_model_negbin_comp <- stan_model(file = "Stan_models/NBA_model_negbin.stan")

stan_fit_negbin <- sampling(stan_model_negbin_comp, data = stan_data, iter = n_sims)
```

```{r __analyze Stan Negative Binomial model}
print(stan_fit_negbin, pars = c("team_skill", "home_court_advantage", "precision"))
```

```{r __posterior predictive check Negative Binomial model}
y_rep_negbin <- as.matrix(stan_fit_negbin, pars = c("away_score_rep", "home_score_rep", "score_difference_rep"))

ppc_dens_overlay(y = game_data$pts_away, yrep = y_rep_negbin[1:400, 1:N_games])
ppc_dens_overlay(y = game_data$pts_home, yrep = y_rep_negbin[1:400, (N_games + 1):(2 * N_games)])
ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_negbin[1:400, (2 * N_games + 1):(3 * N_games)])
```

It's difficult to say whether this fit is better than the Poisson model. While the points scored seem to fit even better, the difference in points scored seems to be a worse fit. This model predicts a bigger probability for very extreme outcomes.

Instead of continuing with the Negative Binomial model, we'll try to modify the Poisson model. A reason why we may miss some variation is that different teams might have different levels of home-court advantage. Maybe we just need a home court advantage vector in addition to the skill vector.

Another thing that may be improved is that we don't use the overtime variable anymore - in overtime games the score tends to be higher since there are more minutes in such a game. We might try to model points per minute instead of points per game.

Finally, we might introduce separate parameters for offensive and defensive skill.

```{r __compile and fit Stan Poisson Home Vector model, include = FALSE}
stan_model_poisson_hv_comp <- stan_model(file = "Stan_models/NBA_model_poisson_home_vector.stan")

stan_fit_poisson_hv <- sampling(stan_model_poisson_hv_comp, data = stan_data, iter = n_sims, chains = 4,
                                # specifying initial values because they get rejected otherwise
                                # this SHOULD be fixed by modeling on the log-scale
                                # but we're lazy right now and won't change the model.
                                init = list(list(home_court_advantage_raw = rep(0, N_teams),
                                                 mu_home = 1),
                                            list(home_court_advantage_raw = rep(1, N_teams),
                                                 mu_home = 0),
                                            list(home_court_advantage_raw = rep(2, N_teams),
                                                 mu_home = 0),
                                            list(home_court_advantage_raw = rep(3, N_teams),
                                                 mu_home = 0)
                                            )
                                )
```

```{r __analyze Stan Poisson Home Vector model}
print(stan_fit_poisson_hv, pars = c("team_skill", "home_court_advantage"))
```

We see quite a different story from earlier - here there seems to be a much clearer variation in home court advantage for different teams. In the worst case this is worth less than two points and in the best case this is worth more than 4.5 points. Maybe the reason this wasn't obvious with the score difference model is that teams may play at a different pace at home or on the road. As a result they may score more points but the opponents as well. We'll keep this in mind for potential model expansion later.

```{r __posterior predictive check Poisson Home Vector model}
y_rep_poisson_hv <- as.matrix(stan_fit_poisson_hv, pars = c("away_score_rep", "home_score_rep", "score_difference_rep"))

ppc_dens_overlay(y = game_data$pts_away, yrep = y_rep_poisson_hv[1:400, 1:N_games])
ppc_dens_overlay(y = game_data$pts_home, yrep = y_rep_poisson_hv[1:400, (N_games + 1):(2 * N_games)])
ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_poisson_hv[1:400, (2 * N_games + 1):(3 * N_games)])
```

Now let's also fit a points per minute model, based on the home vector model. In this model the hidden "skill" parameter determines scoring ability per minute.

```{r __compile and fit Stan Poisson Minutes model, include = FALSE}
stan_model_pois_min <- stan_model(file = "Stan_models/NBA_model_poisson_minutes.stan")

stan_fit_pois_min <- sampling(stan_model_pois_min, data = stan_data, iter = n_sims, chains = 4, 
                                # specifying initial values because they get rejected otherwise
                                # this SHOULD be fixed by modeling on the log-scale
                                # but we're lazy right now and won't change the model.
                                init = list(list(home_court_advantage_raw = rep(0, N_teams),
                                                 mu_home = 1),
                                            list(home_court_advantage_raw = rep(1, N_teams),
                                                 mu_home = 0),
                                            list(home_court_advantage_raw = rep(2, N_teams),
                                                 mu_home = 0),
                                            list(home_court_advantage_raw = rep(3, N_teams),
                                                 mu_home = 0)
                                            )
                              )
```

```{r __analyze Stan Poisson Minutes model}
print(stan_fit_pois_min, pars = c("team_skill", "home_court_advantage"))
```

Convergence diagnostics look fine.

```{r __posterior predictive check Poisson Minutes model}
y_rep_pois_min <- as.matrix(stan_fit_pois_min, pars = c("away_score_rep", "home_score_rep", "score_difference_rep"))

ppc_dens_overlay(y = game_data$pts_away, yrep = y_rep_pois_min[1:400, 1:N_games])
ppc_dens_overlay(y = game_data$pts_home, yrep = y_rep_pois_min[1:400, (N_games + 1):(2 * N_games)])
ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_pois_min[1:400, (2 * N_games + 1):(3 * N_games)])
```

Posterior predictive plots also look fine. 

Next, we'll build a model that accounts for offensive and defensive skill. Here we'll model the parameters on the log scale so that the sampler doesn't need arbitrary initial values. This comes with a major disadvantage though; the parameters are now multiplicative and less interpretable.

```{r __compile and fit Stan Poisson Offense Defense model, include = FALSE}
stan_model_pois_offdef <- stan_model(file = "Stan_models/NBA_model_poisson_offdef.stan")

stan_fit_pois_offdef <- sampling(stan_model_pois_offdef, data = stan_data, iter = 2 * n_sims, chains = 4,
                                # specifying initial values because they get rejected otherwise
                                # this SHOULD be fixed by modeling on the log-scale
                                # but we're lazy right now and won't change the model.
                                init = list(list(off_skill = rep(10, N_teams)),
                                            list(off_skill = rep(20, N_teams)),
                                            list(off_skill = rep(30, N_teams)),
                                            list(off_skill = rep(40, N_teams))
                                            )
                                )
```

```{r __analyze Stan Poisson Offense Defense model}
print(stan_fit_pois_offdef, pars = c("off_skill", "def_skill", "home_court_advantage"))

check_hmc_diagnostics(stan_fit_pois_offdef)
```

Convergence for the skill parameters is good, but effective sample size is a lot smaller. We see home court advantage be about half of what it was in previous models, which makes sense since it is counted double here. Here we can see that Utah is a great defensive team, and Golden State and Houston are great offensive teams. Still, we see many teams defend better than Golden State, so this probably is mainly a function of how fast a team plays. We could potentially still improve this model by adjusting for pace.

```{r __posterior predictive check Poisson Offense Defense model}
y_rep_pois_offdef <- as.matrix(stan_fit_pois_offdef, pars = c("away_score_rep", "home_score_rep", "score_difference_rep"))

ppc_dens_overlay(y = game_data$pts_away, yrep = y_rep_pois_offdef[1:400, 1:N_games])
ppc_dens_overlay(y = game_data$pts_home, yrep = y_rep_pois_offdef[1:400, (N_games + 1):(2 * N_games)])
ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_pois_offdef[1:400, (2 * N_games + 1):(3 * N_games)])
```

Predictive checks mostly seem fine, but we still see more density in the middle of the distribution than this model expects.


## Time-series structure

So far we have only fit the model on the entire season at once. Another approach would be to allow the skill of the teams to vary over time. This makes it possible to estimate the form of a team at various moments in the season.


```{r __set up data for Stan time series modeling}
stan_data_ts <- list(
                    N_games = N_games,
                    N_teams = length(team_mapping_table$team_id),
                    away_points = game_data$pts_away,
                    home_points = game_data$pts_home,
                    overtime = game_data$overtime,
                    away_team_id = game_data$away_team_id,
                    home_team_id = game_data$home_team_id,
                    N_games_per_team = N_games_per_team
                  )

n_sims_ts <- 10000
```


### Time-series model for score difference

First we'll revisit the simpler normal model for score differences with home court, but now we'll update our estimate of the teams' skill parameters after each game.

```{r __compile and fit Stan time-series normal model, include = FALSE}
stan_model_norm_ts_comp <- stan_model(file = "Stan_models/NBA_model_normal_ts.stan")

stan_fit_norm_ts <- sampling(stan_model_norm_ts_comp, data = stan_data_ts, iter = n_sims_ts,
                          control = list(max_treedepth = 15))
```


```{r __analyze Stan time-series normal model}
print(stan_fit_norm_ts, pars = c("home_court_advantage", "score_scale", "init_scale", "update_scale"))

check_hmc_diagnostics(stan_fit_norm_ts)
```

```{r __further analyze Stan time-series normal model}
# options(max.print = 99999)
print(stan_fit_norm_ts, pars = "team_skill")
# options(max.print = 1000)
```

```{r __posterior predictive check time-series normal model}
y_rep_norm_ts <- as.matrix(stan_fit_norm_ts, pars = "score_difference_rep")

ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_norm_ts[1:400, ])
```

The number of effective samples for update_scale is relatively low, but other than that the fit seems to be quite good. Even the team skill parameters (as far as we can see)

Now let's see how team skills evolve over time.


```{r __extract skill over time for normal model}
team_skill <- extract(stan_fit_norm_ts, pars = "team_skill", permuted = TRUE)
team_skill <- team_skill[[1]]
mean_skill <- apply(team_skill, c(2, 3), mean) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
median_skill <- apply(team_skill, c(2, 3), median) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
upper_skill <- apply(team_skill, c(2, 3), quantile, probs = c(.96)) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
lower_skill <- apply(team_skill, c(2, 3), quantile, probs = c(.04)) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
```

```{r __analyze skill over time for normal model}
mean_skill <- mean_skill %>% gather(key = "game", value = "skill", -team_name) %>%
  mutate(game_number = as.numeric(substring(game, first = 2)))

 ranked_teams <- mean_skill %>% group_by(team_name) %>% 
  summarize(mean_skill = mean(skill),
            median_skill = median(skill),
            max_skill = max(skill),
            min_skill = min(skill)) %>%
  arrange(desc(mean_skill))
 
print(ranked_teams, n = 30, digits = 2)

mean_skill %>% filter(team_name %in% ranked_teams$team_name[1:6]) %>% 
  ggplot(mapping = aes(x = game_number, y = skill, color = team_name)) +
  geom_line() + scale_color_brewer(type = "qual", palette = 3) +
  ggtitle("Top Teams in 2016-2017")

mean_skill %>% filter(team_name %in% ranked_teams$team_name[25:30]) %>% 
  ggplot(mapping = aes(x = game_number, y = skill, color = team_name)) +
  geom_line() + scale_color_brewer(type = "qual", palette = 3) +
  ggtitle("Bottom Teams in 2016-2017")


```


### Time-series model for score

Like before we'll use a Poisson distribution to model points scored by away and home teams directly.

THIS MODEL STILL NEEDS TO BE COMPLETED.


```{r __compile and fit Stan time-series Poisson model, include = FALSE}
stan_model_pois_ts_comp <- stan_model(file = "Stan_models/NBA_model_pois_ts.stan")

stan_fit_pois_ts <- sampling(stan_model_pois_ts_comp, data = stan_data_ts, iter = n_sims_ts,
                          control = list(max_treedepth = 15))
```


```{r __analyze Stan time-series Poisson model}
print(stan_fit_pois_ts, pars = c("home_court_advantage", "score_scale", "init_scale", "update_scale"))

check_hmc_diagnostics(stan_fit_pois_ts)
```

```{r __further analyze Stan time-series Poisson model}
# options(max.print = 99999)
print(stan_fit_pois_ts, pars = "team_skill")
# options(max.print = 1000)
```

```{r __posterior predictive check time-series Poisson model}
y_rep_pois_ts <- as.matrix(stan_fit_pois_ts, pars = "score_difference_rep")

ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_pois_ts[1:400, ])
```


```{r __extract skill over time for Poisson model}
team_skill <- extract(stan_fit_pois_ts, pars = "team_skill", permuted = TRUE)
team_skill <- team_skill[[1]]
mean_skill <- apply(team_skill, c(2, 3), mean) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
median_skill <- apply(team_skill, c(2, 3), median) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
upper_skill <- apply(team_skill, c(2, 3), quantile, probs = c(.96)) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
lower_skill <- apply(team_skill, c(2, 3), quantile, probs = c(.04)) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
```

```{r __analyze skill over time for Poisson model}
mean_skill <- mean_skill %>% gather(key = "game", value = "skill", -team_name) %>%
  mutate(game_number = as.numeric(substring(game, first = 2)))

 ranked_teams <- mean_skill %>% group_by(team_name) %>% 
  summarize(mean_skill = mean(skill),
            median_skill = median(skill),
            max_skill = max(skill),
            min_skill = min(skill)) %>%
  arrange(desc(mean_skill))
 
print(ranked_teams, n = 30, digits = 2)

mean_skill %>% filter(team_name %in% ranked_teams$team_name[1:6]) %>% 
  ggplot(mapping = aes(x = game_number, y = skill, color = team_name)) +
  geom_line() + scale_color_brewer(type = "qual", palette = 3) +
  ggtitle("Top Teams in 2016-2017")

mean_skill %>% filter(team_name %in% ranked_teams$team_name[25:30]) %>% 
  ggplot(mapping = aes(x = game_number, y = skill, color = team_name)) +
  geom_line() + scale_color_brewer(type = "qual", palette = 3) +
  ggtitle("Bottom Teams in 2016-2017")


```




# Predicting new games


So far we have checked the models by seeing how well they re-create the original data through simulations. Another way to check is to fit the models on part of the data and see if you can predict the outcomes of other games. Because of the time-series nature of the data we will only fit the models on sequential data and then predict on newer data.

For now we'll just split the season into a training period and then a prediction period - can we predict the next couple of games after training the model?


```{r __additional data manipulation for predictions}
# arbitrary split of the season

N_games_played <- 200 # we'll fit the model on this many games
N_new_games <- N_games - N_games_played
#N_new_games <- 10 # we'll predict this many games after training the model

game_data_played <- game_data[1:N_games_played, ]
game_data_new <- game_data[(N_games_played + 1):(N_games_played + N_new_games) ,]

#game_data_pred <- lookup_amount_games_played(game_data, team_mapping_table, N_games_per_team)

stan_data_pred <- list(
                    N_games = N_games_played,
                    N_new_games = N_new_games,
                    N_teams = N_teams,
                    away_points = game_data_played$pts_away,
                    home_points = game_data_played$pts_home,
                    overtime = game_data_played$overtime,
                    away_team_id = game_data_played$away_team_id,
                    home_team_id = game_data_played$home_team_id,
                    away_id_new = game_data_new$away_team_id,
                    home_id_new = game_data_new$home_team_id
                  )

n_sims_pred <- 4000

```

```{r __compile and fit predictive normal model, include = FALSE}
stan_pred_normal_comp <- stan_model(file = "Stan_models/NBA_pred_normal.stan")

stan_pred_normal_fit <- sampling(stan_pred_normal_comp, data = stan_data_pred, iter = n_sims_pred)
```

```{r __analyze output of predictive normal model}
print(stan_pred_normal_fit, pars = c("score_difference_pred"))

game_data_new %>% select(date, away_team, home_team, pts_away, pts_home, overtime, score_diff) %>% print(n = 10)

score_diff_predictions <- as.matrix(stan_pred_normal_fit, pars = c("score_difference_pred"))
score_diff_actual <- game_data_new$score_diff

cum_probs <- compare_predictions_to_actual(score_diff_predictions, score_diff_actual)
# print(cum_probs)
quantile(cum_probs, probs = c(0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99))

predicted_outcome <- compute_predicted_outcome(score_diff_predictions)
actual_outcome <- sign(score_diff_actual)

prediction_table <- matrix(c(predicted_outcome, actual_outcome, predicted_outcome == actual_outcome), ncol = 3)
prediction_quality <- mean(abs(predicted_outcome - actual_outcome) / 2) # lower is better

print(prediction_quality)
# print(prediction_table)
```

After training on 100 games and predicting 10 games, we see multiple games where the actual score difference is outside the 95% posterior interval for the predicted score difference. After training on 200 games we still had this problem. We see a couple of extreme outcomes (above 98% cumulative probability). Still, this may be a result of the model used. 

The framework seems to work though, so now let's try to use some more sophisticated models for prediction.

```{r __compile and fit predictive normal home model, include = FALSE}
stan_pred_normal_home_comp <- stan_model(file = "Stan_models/NBA_pred_normal_home.stan")

stan_pred_normal_home_fit <- sampling(stan_pred_normal_home_comp, data = stan_data_pred, iter = n_sims_pred)
```

```{r __analyze output of predictive normal home model}
print(stan_pred_normal_home_fit, pars = c("score_difference_pred", "home_court_advantage"))

game_data_new %>% select(date, away_team, home_team, pts_away, pts_home, overtime, score_diff) %>% print(n = 10)

score_diff_predictions <- as.matrix(stan_pred_normal_home_fit, pars = c("score_difference_pred"))
score_diff_actual <- game_data_new$score_diff

cum_probs <- compare_predictions_to_actual(score_diff_predictions, score_diff_actual)
# print(cum_probs)
quantile(cum_probs, probs = c(0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99))

predicted_outcome <- compute_predicted_outcome(score_diff_predictions)
actual_outcome <- sign(score_diff_actual)

prediction_table <- matrix(c(predicted_outcome, actual_outcome, predicted_outcome == actual_outcome), ncol = 3)
prediction_quality <- mean(abs(predicted_outcome - actual_outcome) / 2) # lower is better

print(prediction_quality)
# print(prediction_table)
```

We still see quite a few extreme outcomes, so let's try to make predictions using a t model.


```{r __compile and fit predictive t home model, include = FALSE}
stan_pred_t_home_comp <- stan_model(file = "Stan_models/NBA_pred_t_home.stan")

stan_pred_t_home_fit <- sampling(stan_pred_t_home_comp, data = stan_data_pred, iter = n_sims_pred)
```

```{r __analyze output of predictive t home model}
print(stan_pred_t_home_fit, pars = c("deg_free", "score_difference_pred", "home_court_advantage"))

game_data_new %>% select(date, away_team, home_team, pts_away, pts_home, overtime, score_diff) %>% print(n = 10)

score_diff_predictions <- as.matrix(stan_pred_t_home_fit, pars = c("score_difference_pred"))
score_diff_actual <- game_data_new$score_diff

cum_probs <- compare_predictions_to_actual(score_diff_predictions, score_diff_actual)
# print(cum_probs)
quantile(cum_probs, probs = c(0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99))

predicted_outcome <- compute_predicted_outcome(score_diff_predictions)
actual_outcome <- sign(score_diff_actual)

prediction_table <- matrix(c(predicted_outcome, actual_outcome, predicted_outcome == actual_outcome), ncol = 3)
prediction_quality <- mean(abs(predicted_outcome - actual_outcome) / 2) # lower is better

print(prediction_quality)
# print(prediction_table)
```


The quantiles seem slightly better calibrated, but the win/loss predictions aren't when compared to the normal model with home court advantage. The degrees of freedom parameter is estimated to be quite low; this is evidence against the normal model.



