---
title: "NBA Game Predictions Part 1"
output: html_notebook
---


$$
y_i \sim \mathrm{N}(\mu_i, \sigma) \\
\mu_i = \alpha_{k[i]} + \beta_{k[i]} * x_i \\
\left[\begin{array}{cc}
\alpha_k\\
\beta_k
\end{array}\right] 
\sim \mathrm{N}_2(\boldsymbol{\mu, {S}}) \\
\alpha \sim \mathrm{N}(0, 10) \\
\beta \sim \mathrm{N}(0, 1) \\
\boldsymbol{S} = 
\left(\begin{array}{cc}
\sigma_\alpha & 0 \\
0 & \sigma_\beta
\end{array}\right) C 
\left(\begin{array}{cc}
\sigma_\alpha & 0 \\
0 & \sigma_\beta
\end{array}\right) \\
(\sigma_\alpha, \sigma_\beta) \sim \mathrm{HalfCauchy}(0, 1) \\
C \sim \mathrm{LKJcorr}(2)
$$


# Introduction

In this series of notebooks we will build some Bayesian models to model the outcome of NBA games. We will then try to use these models to predict the outcomes of new NBA games.

In this part we will try to model the winner of a game. We'll start with the simplest possible model and slowly add complexity to model specific aspects of a basketball game.

In the next parts we will try a number of alternative approaches and eventually see which one works best.


# Introduction to Inference: Probablistic description of a basketball game

The outcome of a basketball game is uncertain. While we might know in advance which team is more likely to win, we can never be sure until the final buzzer has sounded. Because of this uncertainty we will describe basketball games using *probabilistic models*. Describing a game like this will allow us to then perform inference over the quantities of interest.

A very simple example of how to model a basketball game is as follows:

- every team has a certain skill level, which we cannot observe, which we will describe using a real-valued parameter
- the outcome of a basketball game is a random process which uses these skill parameters of the two teams playing, as well as potentially other parameters

A very basic process that uses these to assumptions is to model the outcome of a basketball game as follows:

$$
w_i \sim \mathrm{Bernoulli}(p_i) \\
p_i = \mathrm{Logit^{-1}}({s_{1i} - s_{2i}}) \\
s_{1i}, s_{2i} \sim \mathrm{N}(0, 1) \\
$$

Here $w_i$ denotes the winner of game $i$, and $s_{1i}$ and $s_{2i}$ denote the skill parameters of the two teams in game $i$. This essentially means that each basketball game is simplified to a single coinflip, where the probability of landing heads (i.e. the home team winning) is a function of the two skill levels of the teams.

A similar model for the outcome of a basketball game is this.

$$
w_i \sim \mathrm{Bernoulli}(p_i) \\
p_i = \mathrm{InverseNormal}(\frac{s_{1i} - s_{2i}}{\sigma}) \\
s_{1i}, s_{2i} \sim \mathrm{N}(0, 1) \\
\sigma \sim \mathrm{HalfNormal}(0, 1)
$$

where $w_i$ denotes the winner of game $i$, $s_{1i}$ and $s_{2i}$ denote the skill parameters of the two teams in game $i$, and $\sigma$ denotes the amount of uncertainty present in a basketball game. The priors essentially encode that we don't know before the game which team is better, so both hidden skill parameters are drawn from the same distribution centered at zero.

This will be the our most basic model of a basketball game.

When our assumptions are explicitly encoded into a model, we can combine the model with observations to learn more about the parameters. This process is called *inference*.

For example, if team 1 won the game, we would infer that $s_1$ is likely greater than $s_2$. In particular, we started off with a probability distribution for these parameters which was identical, and combining this distribution with data will yield a new probability distribution for the parameters, which is called the *posterior* distribution. In these notebooks we will use a language called Stan to calculate the posterior distribution for us.

Still, the amount that can be learned from a single game is limited. In our NBA example, we'd like to analyze the outcomes of many games, so that we can more accurately learn the skill levels of all the teams.


# Initial setup

```{r load packages and further setup, message=FALSE, include=FALSE}
library(tidyverse)
library(rstan)
library(loo)
library(bayesplot)

theme_set(bayesplot::theme_default())

options(mc.cores = parallel::detectCores())

source("NBA_utilities.R")

set.seed(12)
```


```{r load data and some data manipulation}
game_data <- read_csv("NBA Game Data 20162017.csv")
game_data <- game_data %>% 
                select(date = Date, away_team = "Visitor/Neutral", home_team = "Home/Neutral", pts_away = PTS_A, pts_home = PTS_H, overtime = OT) %>%
                mutate(overtime = ifelse (is.na(overtime), 0L, 
                                          ifelse(stringr::str_length(overtime) == 2, 1L, as.integer(substring(overtime, 1, 1)))),
                       score_diff = ifelse(overtime == 0, pts_away - pts_home, 0L),
                       home_win = ifelse(pts_away - pts_home < 0, 1L, 0L))

# create a single identifier used for both away and home teams
unique_teams <- sort(unique(c(game_data$away_team, game_data$home_team)))
team_mapping_table <- tibble(team_name = unique_teams, team_id = 1:length(unique_teams))

team_id <- map_int(c(game_data$away_team, game_data$home_team), lookup_team_id, mapping_table = team_mapping_table)
N_games <- nrow(game_data)
N_teams = length(team_mapping_table$team_id)
game_data <- game_data %>% mutate(away_team_id = team_id[1:N_games], home_team_id = team_id[(N_games + 1):(2 * N_games)])
N_games_per_team <- 82
n_sims <- 4000
```


# Some quick data exploration


```{r}
str(game_data)
```


# Modeling game outcomes with Stan

```{r set up data for Stan}
stan_data <- list(
                    N_games = N_games,
                    N_teams = N_teams,
                    home_win = game_data$home_win,
                    away_team_id = game_data$away_team_id,
                    home_team_id = game_data$home_team_id
                  )
```





## Modeling the season as a whole

First we'll try to fit a model to all games at once.

```{r compile and fit base bernoulli logit model, include = FALSE}
bern_logit_stan <- stan_model(file = "Stan_models/NBA_base_bernoulli_logit_win_prob.stan")

fit_bern_logit <- sampling(bern_logit_stan, data = stan_data, iter = n_sims)
```

```{r analyze base Stan model}
print(fit_bern_logit, pars = c("team_skill"))
```

The base model shows a very bad fit. The reason is that the model is non-identifiable: since we only model the difference in points scored, we can add an arbitrary constant to home skill and away skill for all teams in the dataset and this will lead to the exact same inference. We'll quickly fix this by adding some weak priors.

```{r compile and fit weak prior normal Stan model, include = FALSE}
stan_model_base_comp <- stan_model(file = "Stan_models/NBA_base_model_normal_weak_priors.stan")

stan_fit_base <- sampling(stan_model_base_comp, data = stan_data, iter = n_sims)
```

```{r analyze weak prior normal Stan model}
print(stan_fit_base, pars = c("team_skill", "stdev"))
```

This looks much better - the convergence diagnostics show no problems now. We see the Golden State Warriors are the best team here and the Brooklyn Nets are the worst. This is not a surprise - these were the teams that won the most / least games, respectively. Let's see how well we re-predict the game data.

```{r posterior predictive check base model}
y_rep_base <- as.matrix(stan_fit_base, pars = "score_difference_rep")

ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_base[1:400, ])
```



### Models based on Elo / TrueSkill - modeling game winner





