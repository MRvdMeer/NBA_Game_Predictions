---
title: "NBA Game Predictions Part 1"
output: html_notebook
---


$$
y_i \sim \mathrm{N}(\mu_i, \sigma) \\
\mu_i = \alpha_{k[i]} + \beta_{k[i]} * x_i \\
\left[\begin{array}{cc}
\alpha_k\\
\beta_k
\end{array}\right] 
\sim \mathrm{N}_2(\boldsymbol{\mu, {S}}) \\
\alpha \sim \mathrm{N}(0, 10) \\
\beta \sim \mathrm{N}(0, 1) \\
\boldsymbol{S} = 
\left(\begin{array}{cc}
\sigma_\alpha & 0 \\
0 & \sigma_\beta
\end{array}\right) C 
\left(\begin{array}{cc}
\sigma_\alpha & 0 \\
0 & \sigma_\beta
\end{array}\right) \\
(\sigma_\alpha, \sigma_\beta) \sim \mathrm{HalfCauchy}(0, 1) \\
C \sim \mathrm{LKJcorr}(2)
$$


# Introduction

In this series of notebooks we will build some Bayesian models to model the outcome of NBA games. We will then try to use these models to predict the outcomes of new NBA games.

In this part we will try to model the winner of a game. We'll start with the simplest possible model and slowly add complexity to model specific aspects of a basketball game.

In the next parts we will try a number of alternative approaches and eventually see which one works best.


# Introduction to Inference: Probablistic description of a basketball game

The outcome of a basketball game is uncertain. While we might know in advance which team is more likely to win, we can never be sure until the final buzzer has sounded. Because of this uncertainty we will describe basketball games using *probabilistic models*. Describing a game like this will allow us to then perform inference over the quantities of interest.

A very simple example of how to model a basketball game is as follows:

- every team has a certain skill level, which we cannot observe, which we will describe using a real-valued parameter
- the outcome of a basketball game is a random process which uses these skill parameters of the two teams playing, as well as potentially other parameters

We will use the Normal distribution to encode the outcome of a basketball game as follows:


$$
w_i \sim \mathrm{Bernoulli}(p_i) \\
p_i = \mathrm{InverseNormal}(\frac{s1_i - s2_i}{\sigma}) \\
s1_i, s2_i \sim \mathrm{N}(0, 1) \\
\sigma \sim \mathrm{HalfNormal}(0, 1)
$$

where $w_i$ denotes the winner of game $i$, $s1_i$ and $s2_i$ denote the skill parameters of the two teams in game $i$, and $\sigma$ denotes the amount of uncertainty present in a basketball game. The priors essentially encode that we don't know before the game which team is better, so both hidden skill parameters are drawn from the same distribution centered at zero.

This will be the our most basic model of a basketball game.

When our assumptions are explicitly encoded into a model, we can combine the model with observations to learn more about the parameters. This process is called *inference*.

For example, if team 1 won the game, we would infer that $s1$ is likely greater than $s2$. In particular, we started off with a probability distribution for these parameters which was identical, and combining this distribution with data will yield a new probability distribution for the parameters, which is called the *posterior* distribution. In these notebooks we will use a language called Stan to calculate the posterior distribution for us.

Still, the amount that can be learned from a single game is limited. In our NBA example, we'd like to analyze the outcomes of many games, so that we can more accurately learn the skill levels of all the teams.


# Initial setup

```{r load packages and further setup, message=FALSE, include=FALSE}
library(tidyverse)
library(rstan)
library(loo)
library(bayesplot)

theme_set(bayesplot::theme_default())

options(mc.cores = parallel::detectCores())

source("NBA_utilities.R")

set.seed(12)
```


```{r load data and some data manipulation}
game_data <- read_csv("NBA Game Data 20162017.csv")
game_data <- game_data %>% 
                select(date = Date, away_team = "Visitor/Neutral", home_team = "Home/Neutral", pts_away = PTS_A, pts_home = PTS_H, overtime = OT) %>%
                mutate(overtime = ifelse (is.na(overtime), 0L, 
                                          ifelse(stringr::str_length(overtime) == 2, 1L, as.integer(substring(overtime, 1, 1)))),
                       score_diff = ifelse(overtime == 0, pts_away - pts_home, 0L),
                       home_win = ifelse(pts_away - pts_home < 0, 1L, 0L))

# create a single identifier used for both away and home teams
unique_teams <- sort(unique(c(game_data$away_team, game_data$home_team)))
team_mapping_table <- tibble(team_name = unique_teams, team_id = 1:length(unique_teams))

team_id <- map_int(c(game_data$away_team, game_data$home_team), lookup_team_id, mapping_table = team_mapping_table)
N_games <- nrow(game_data)
N_teams = length(team_mapping_table$team_id)
game_data <- game_data %>% mutate(away_team_id = team_id[1:N_games], home_team_id = team_id[(N_games + 1):(2 * N_games)])
N_games_per_team <- 82
n_sims <- 4000

```


# Some quick data exploration

```{r}
str(game_data)
```


# Modeling game outcomes with Stan

```{r set up data for Stan}
stan_data <- list(
                    home_win = home_win,
                    home_skill_est = home_skill_est,
                    home_skill_uncertainty = home_skill_uncertainty,
                    away_skill_est = away_skill_est,
                    away_skill_uncertainty = away_skill_uncertainty,
                    game_uncertainty = game_uncertainty
                  )
```





## Modeling the season as a whole

First we'll try to fit a model to all games at once. 

### Models based on Elo / TrueSkill - modeling game winner





