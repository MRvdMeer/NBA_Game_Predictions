---
title: "NBA Game Predictions Part 1"
output: html_notebook
---


$$
y_i \sim \mathrm{N}(\mu_i, \sigma) \\
\mu_i = \alpha_{k[i]} + \beta_{k[i]} * x_i \\
\left[\begin{array}{cc}
\alpha_k\\
\beta_k
\end{array}\right] 
\sim \mathrm{N}_2(\boldsymbol{\mu, {S}}) \\
\alpha \sim \mathrm{N}(0, 10) \\
\beta \sim \mathrm{N}(0, 1) \\
\boldsymbol{S} = 
\left(\begin{array}{cc}
\sigma_\alpha & 0 \\
0 & \sigma_\beta
\end{array}\right) C 
\left(\begin{array}{cc}
\sigma_\alpha & 0 \\
0 & \sigma_\beta
\end{array}\right) \\
(\sigma_\alpha, \sigma_\beta) \sim \mathrm{HalfCauchy}(0, 1) \\
C \sim \mathrm{LKJcorr}(2)
$$


# Introduction

In this series of notebooks we will build some Bayesian models to model the outcome of NBA games. We will then try to use these models to predict the outcomes of new NBA games.

In this part we will try to model the winner of a game. We'll start with the simplest possible model and slowly add complexity to model specific aspects of a basketball game.

In the next parts we will try a number of alternative approaches and eventually see which one works best.


# Introduction to Inference: Probablistic description of a basketball game

The outcome of a basketball game is uncertain. While we might know in advance which team is more likely to win, we can never be sure until the final buzzer has sounded. Because of this uncertainty we will describe basketball games using *probabilistic models*. Describing a game like this will allow us to then perform inference over the quantities of interest.

A very simple set of assumptions of how to model a basketball game is as follows:

- every team has a certain skill level, which we cannot observe, which we will describe using a real-valued parameter
- the outcome of a basketball game is a random process that only depends on the difference between the skill levels of the two teams

Putting these assumptions into a set of equations might look as follows:

$$
w_i \sim \mathrm{Bernoulli}(p_i) \\
p_i = \mathrm{Logit^{-1}}({s_{1i} - s_{2i}}) \\
s_{1i}, s_{2i} \sim \mathrm{N}(0, 1) \\
$$

Here $w_i$ denotes the winner of game $i$, and $s_{1i}$ and $s_{2i}$ denote the skill parameters of the two teams in game $i$. This essentially means that each basketball game is simplified to a single coinflip, where the probability of landing heads (i.e. the home team winning) is a function of the two skill levels of the teams.

This will be the our most basic model of a basketball game.

When our assumptions are explicitly encoded into a model, we can combine the model with observations to learn more about the parameters. This process is called *inference*.

For example, if team 1 won the game, we would infer that $s_1$ is likely greater than $s_2$. In particular, we started off with a probability distribution for these parameters which was identical, and combining this distribution with data will yield a new probability distribution for the parameters, which is called the *posterior* distribution. In these notebooks we will use a language called Stan to calculate the posterior distribution for us.

Still, the amount that can be learned from a single game is limited. In our NBA example, we'd like to analyze the outcomes of many games, so that we can more accurately learn the skill levels of all the teams.


# Initial setup

```{r load packages and further setup, message=FALSE, include=FALSE}
library(tidyverse)
library(rstan)
library(bayesplot)

theme_set(bayesplot::theme_default())

options(mc.cores = parallel::detectCores())

source("./NBA_utilities.R")

set.seed(12)
```


```{r load data and some data manipulation, include = FALSE}
game_data <- read_csv("./NBA Game Data 20162017.csv")
game_data <- game_data %>% 
                select(date = Date, away_team = "Visitor/Neutral", home_team = "Home/Neutral", pts_away = PTS_A, pts_home = PTS_H, overtime = OT) %>%
                mutate(overtime = ifelse (is.na(overtime), 0L, 
                                          ifelse(stringr::str_length(overtime) == 2, 1L, as.integer(substring(overtime, 1, 1)))),
                       score_diff = ifelse(overtime == 0, pts_away - pts_home, 0L),
                       home_win = ifelse(pts_away - pts_home < 0, 1L, 0L))

# create a single identifier used for both away and home teams
unique_teams <- sort(unique(c(game_data$away_team, game_data$home_team)))
team_mapping_table <- tibble(team_name = unique_teams, team_id = 1:length(unique_teams))

team_id <- map_int(c(game_data$away_team, game_data$home_team), lookup_team_id, mapping_table = team_mapping_table)
N_games <- nrow(game_data)
N_teams = length(team_mapping_table$team_id)
game_data <- game_data %>% mutate(away_team_id = team_id[1:N_games], home_team_id = team_id[(N_games + 1):(2 * N_games)])
N_games_per_team <- 82
n_sims <- 10000
```


# Some quick data exploration


```{r}
str(game_data)
```


# Modeling game outcomes with Stan

```{r set up data for Stan}
stan_data <- list(
                    N_games = N_games,
                    N_teams = N_teams,
                    home_win = game_data$home_win,
                    away_team_id = game_data$away_team_id,
                    home_team_id = game_data$home_team_id
                  )
```


## Modeling the season as a whole


```{r compile and fit base logit model, include = FALSE}
logit_winprob_stan <- stan_model(file = "./Stan_models/NBA_base_logit_win_prob.stan")

fit_logit_winprob <- sampling(logit_winprob_stan, data = stan_data, iter = n_sims)
```

```{r analyze base logit model}
print(fit_logit_winprob, pars = c("team_skill"))
```

This table shows the estimated team skills for all the teams. We can see that team 10 (the Golden State Warriors) has the highest estimated skill and team 3 (the Brooklyn Nets) has the lowest. This is not a surprise - these are the teams with the highest and lowest win totals in the 2016-2017 season, respectively.


```{r posterior predictive check base logit model}
y_rep_logit <- as.matrix(fit_logit_winprob, pars = "home_win_rep")

ppc_dens_overlay(y = game_data$home_win, yrep = y_rep_logit[1:400, ])
```


Here we see that outcomes are less symmetric than in our assumptions - home teams tend to win more than away teams, all else being equal. This should not come as a big surprise, as the home team tends to have a small but noticable advantage in basketball. We'll keep this in mind when trying to improve our model.


### Interpreting parameter values

So far we've looked at the estimated skill values the model, but we haven't really tried to interpret the values of those estimates. For example, we've seen that the Golden State Warriors are estimated to be the best team, but their estimated skill difference over the San Antonio Spurs (team 27) is `1.45 - 1.06 = 0.39`. What does this mean?

First, note that $\mathrm{Logit}(p) = \mathrm{log} \frac{p}{1 - p}$, which is the logarithm of the odds ratio of winning, and so if $s_1 - s_2 = \mathrm{Logit}(p)$ then $\frac{p}{1 - p} = \mathrm{exp}(s_1 - s_2)$. So if $s_1 = s_2$ then $p = 1 - p$ and both teams have a 50 percent probability of winning.

In general, suppose we have team skills $s_1$ and $s_2$, and now team 1 increases its skill by exactly one point, so $s_1' = s_1 + 1$. What does this mean for the probabilities of winning? We have 

$$
\frac{p'}{1 - p'} = \mathrm{exp}(s_1' - s_2) = \mathrm{exp}(s_1 + 1 - s_2) = e \ \mathrm{exp}(s_1 - s_2) = e \ \frac{p}{1 - p}
$$

So adding one point to the skill of a team means its odds ratio of winning are multiplied by $e$, which is roughly 2.7. Below we print a more complete table of win probabilities for a given difference in skill levels.


```{r}
x <- seq(from = -3, to = 3, by = 0.5)

y_logit <- exp(x) / (1 + exp(x))

df <- tibble(skill_diff = x, win_prob = y_logit)

df
```


### Improving the model

As we noticed, teams playing at home tend to win more often than teams playing on the road. We'll add a parameter to the model to take this into account. The mathematical formulation of the model then becomes:

$$
w_i \sim \mathrm{Bernoulli}(p_i) \\
p_i = \mathrm{Logit^{-1}}({s_{1i} + h - s_{2i}}) \\
s_{1i}, s_{2i} \sim \mathrm{N}(0, 1) \\
h \sim \mathrm{N}(0, 1)
$$

where the new parameter $h$ denotes the average home-court advantage in the league.


```{r compile and fit logit homecourt model, include = FALSE}
logit_homecourt_stan <- stan_model(file = "./Stan_models/NBA_logit_homecourt.stan")

fit_logit_homecourt <- sampling(logit_homecourt_stan, data = stan_data, iter = n_sims)
```

```{r analyze logit homecourt model}
print(fit_logit_homecourt, pars = c("team_skill", "home_court_advantage"))
```

Here we see the new predicted skill values of the teams. We also see that playing at home boosts the skill of the home team by approximately 0.40 points, which means its odds ratio of winning are multiplied by about 1.5.


```{r posterior predictive check logit homecourt model}
y_rep_logit_hc <- as.matrix(fit_logit_homecourt, pars = "home_win_rep")

ppc_dens_overlay(y = game_data$home_win, yrep = y_rep_logit_hc[1:400, ])
```

In terms of re-predicting the data, this model does a lot better. One final improvement we might try is to give each team its own home-court advantage. This model can be expressed in the following set of equations.

$$
w_i \sim \mathrm{Bernoulli}(p_i) \\
p_i = \mathrm{Logit^{-1}}({s_{1i} + h_1 - s_{2i}}) \\
s_{1i}, s_{2i} \sim \mathrm{N}(0, 1) \\
\forall j: h_j \sim \mathrm{N}(0, 1)
$$


```{r compile and fit logit home vector model, include = FALSE}
logit_home_vector_stan <- stan_model(file = "./Stan_models/NBA_logit_home_vector.stan")

fit_logit_home_vector <- sampling(logit_home_vector_stan, data = stan_data, iter = n_sims)
```

```{r analyze logit home vector model}
print(fit_logit_home_vector, pars = c("team_skill", "home_court_advantage"))
```

Here we see there is quite some difference in home-court advantage. Some teams become a lot better at home, while for other teams it barely makes a difference (or even seems to hurt in the case of teams 22 and 26, the Orlando Magic and the Sacramento Kings).


```{r posterior predictive check logit home vector model}
y_rep_logit_hv <- as.matrix(fit_logit_home_vector, pars = "home_win_rep")

ppc_dens_overlay(y = game_data$home_win, yrep = y_rep_logit_hv[1:400, ])
```

This model seems about as good as the previous one at re-predicting the data.


## Modeling the season as a time series







