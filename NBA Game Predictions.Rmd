---
title: "NBA Game Predictions"
output: html_notebook
---

```{r load packages and further setup, message=FALSE, include=FALSE}
library(tidyverse)
library(rstan)
library(bayesplot)

theme_set(bayesplot::theme_default())

options(mc.cores = parallel::detectCores())

source("NBA_utilities.R")

set.seed(12)
```

```{r load data and some data manipulation}
game_data <- read_csv("NBA Game Data 20162017.csv")
game_data <- game_data %>% 
                select(date = Date, away_team = "Visitor/Neutral", home_team = "Home/Neutral", pts_away = PTS_A, pts_home = PTS_H, overtime = OT) %>%
                mutate(overtime = ifelse (is.na(overtime), 0L, 
                                          ifelse(stringr::str_length(overtime) == 2, 1L, as.integer(substring(overtime, 1, 1)))),
                       score_diff = ifelse(overtime == 0, pts_away - pts_home, 0L))

# create a single identifier used for both away and home teams
unique_teams <- sort(unique(c(game_data$away_team, game_data$home_team)))
team_mapping_table <- tibble(team_name = unique_teams, team_id = 1:length(unique_teams))

lookup_team_id <- function(team_name_lookup, mapping_table) {
  tmp_table <- filter(mapping_table, team_name == team_name_lookup)
  if (nrow(tmp_table) != 1) {
    stop(paste("lookup-rows should be equal to one but found", nrow(tmp_table), "instead"))
  }
  out <- tmp_table$team_id
  out
}

team_id <- map_int(c(game_data$away_team, game_data$home_team), lookup_team_id, mapping_table = team_mapping_table)
N_games <- nrow(game_data)
game_data <- game_data %>% mutate(away_team_id = team_id[1:N_games], home_team_id = team_id[(N_games + 1):(2 * N_games)])
N_games_per_team <- 82
n_sims <- 4000

```

For now we won't include games that went into overtime, as these could be considered a 'tie'. There were a total of 70 games that went to overtime in the regular season of 2016/2017.

```{r set up data for Stan}
stan_data <- list(
                    N_games = N_games,
                    N_teams = length(team_mapping_table$team_id),
                    away_points = game_data$pts_away,
                    home_points = game_data$pts_home,
                    overtime = game_data$overtime,
                    away_team_id = game_data$away_team_id,
                    home_team_id = game_data$home_team_id
                  )
```


```{r quick summary of the data}
paste("Average score difference:", round(mean(game_data$score_diff), 2))
paste("Standard deviation of score difference:", round(sd(game_data$score_diff), 2))

paste("Average points scored by away team:", round(mean(game_data$pts_away), 2))
paste("Variance of away points:", round(var(game_data$pts_away), 2))
paste("Average points scored by home team:", round(mean(game_data$pts_home), 2))
paste("Variance of home points:", round(var(game_data$pts_home), 2))

summary(game_data)
```

Here we see both the home and away points scored are a bit over-dispersed.

```{r some quick plots}
ggplot(data = game_data, mapping = aes(x = score_diff)) + geom_density(color = "blue")

game_data %>%
  gather(key = "away_home", value = "points", pts_away, pts_home) %>%
  ggplot(aes(x = points, col = away_home)) + geom_density() + scale_color_manual(values = c("orange", "darkblue"))
```


It appears the observed score difference seems to be bimodal, and there appears to be some home-court advantage (more negative score differences than positive). We will use this in the model later.
Also, the distribution of points scored seems relatively bell-shaped, with perhaps a bit more mass towards the tails than we would expect from a normal distribution.


# Modeling the game outcomes


### Model based on the Gelman Worldcup example - modeling score difference

First of all, we'll use a model based on [Andrew Gelman's World Cup example](https://andrewgelman.com/2014/07/15/stan-world-cup-update/). Here we model the score difference in a game using a symmetric distribution centered on the skill difference between the two teams.


```{r compile and fit base Stan model, include = FALSE}
stan_model_bad_comp <- stan_model(file = "Stan_models/NBA_base_model_normal.stan")

stan_fit_bad <- sampling(stan_model_bad_comp, data = stan_data, iter = n_sims)
```

```{r analyze base Stan model}
print(stan_fit_bad, pars = c("team_skill", "stdev"))
```

The base model shows a very bad fit. The reason is that the model is non-identifiable: since we only model the difference in points scored, we can add an arbitrary constant to home skill and away skill for all teams in the dataset and this will lead to the exact same inference. We'll quickly fix this by adding some weak priors.


```{r compile and fit weak prior normal Stan model, include = FALSE}
stan_model_base_comp <- stan_model(file = "Stan_models/NBA_base_model_normal_weak_priors.stan")

stan_fit_base <- sampling(stan_model_base_comp, data = stan_data, iter = n_sims)
```

```{r analyze weak prior normal Stan model}
print(stan_fit_base, pars = c("team_skill", "stdev"))
```

This looks much better - the convergence diagnostics show no problems now. We see the Golden State Warriors are the best team here and the Brooklyn Nets are the worst. This is not a surprise - these were the teams that won the most / least games, respectively. Let's see how well we re-predict the game data.

```{r posterior predictive check base model}
y_rep_base <- as.matrix(stan_fit_base, pars = "score_difference_rep")

ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_base[1:400, ])
```


The replicated datasets seem to re-predict the actual score differences reasonably well, although it seems that the actual data is a bit more skewed to the left (i.e. in favor of the home team). Perhaps the tails also don't work quite well. We'll try to tackle both of these next.


```{r compile and fit t Stan model, include = FALSE}
stan_model_t_comp <- stan_model(file = "Stan_models/NBA_model_t.stan")

stan_fit_t <- sampling(stan_model_t_comp, data = stan_data, iter = n_sims)
```

```{r analyze t Stan model}
print(stan_fit_t, pars = c("team_skill", "scale", "deg_free"))
```

```{r posterior predictive check t model}
y_rep_t <- as.matrix(stan_fit_t, pars = "score_difference_rep")

ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_t[1:400, ])
```

The t-model seems to have fatter tails than necessary for our purposes. For now we'll stick with the normal model.


```{r compile and fit Stan model with home court advantage, include = FALSE}
stan_model_home_comp <- stan_model(file = "Stan_models/NBA_model_normal_home.stan")

stan_fit_home <- sampling(stan_model_home_comp, data = stan_data, iter = n_sims)
```


```{r analyze Stan model with home court advantage}
print(stan_fit_home, pars = c("team_skill", "home_court_advantage", "scale"))
```

It seems home court advantage is worth roughly 3 points in the NBA.


```{r posterior predictive check home court model}
y_rep_home <- as.matrix(stan_fit_home, pars = "score_difference_rep")

ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_home[1:400, ])
```


### Count models for the individual team scores

So far we have modeled the score difference only, but now we'll try to directly model the points scored by the home and away teams. First we'll try a Poisson model. We've seen earlier that the points scored seem slighly over-dispersed, so we'll keep that in mind here.


```{r compile and fit Stan Poisson model, include = FALSE}
stan_model_poisson_comp <- stan_model(file = "Stan_models/NBA_model_poisson.stan")

stan_fit_poisson <- sampling(stan_model_poisson_comp, data = stan_data, iter = n_sims)
```


```{r analyze Stan Poisson model}
print(stan_fit_poisson, pars = c("team_skill", "home_court_advantage"))
```


```{r posterior predictive check Poisson model}
y_rep_poisson <- as.matrix(stan_fit_poisson, pars = c("away_score_rep", "home_score_rep", "score_difference_rep"))

ppc_dens_overlay(y = game_data$pts_away, yrep = y_rep_poisson[1:400, 1:N_games])
ppc_dens_overlay(y = game_data$pts_home, yrep = y_rep_poisson[1:400, (N_games + 1):(2 * N_games)])
ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_poisson[1:400, (2 * N_games + 1):(3 * N_games)])
```

The Poisson model predicts the points scored quite well, and as a result the difference in points scored too. We'll investigate whether a Negative Binomial model does even better - we noticed some over-dispersion earlier and this might help fix that.


```{r compile and fit Stan Negative Binomial model, include = FALSE}
stan_model_negbin_comp <- stan_model(file = "Stan_models/NBA_model_negbin.stan")

stan_fit_negbin <- sampling(stan_model_negbin_comp, data = stan_data, iter = n_sims)
```


```{r analyze Stan Negative Binomial model}
print(stan_fit_negbin, pars = c("team_skill", "home_court_advantage", "precision"))
```


```{r posterior predictive check Negative Binomial model}
y_rep_negbin <- as.matrix(stan_fit_negbin, pars = c("away_score_rep", "home_score_rep", "score_difference_rep"))

ppc_dens_overlay(y = game_data$pts_away, yrep = y_rep_negbin[1:400, 1:N_games])
ppc_dens_overlay(y = game_data$pts_home, yrep = y_rep_negbin[1:400, (N_games + 1):(2 * N_games)])
ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_negbin[1:400, (2 * N_games + 1):(3 * N_games)])
```

It's difficult to say whether this fit is better than the Poisson model. While the points scored seem to fit even better, the difference in points scored seems to be a worse fit. This model predicts a bigger probability for very extreme outcomes.

Instead of continuing along the Negative Binomial path, we'll try to modify the Poisson model. A reason why we may miss some variation is that different teams might have different levels of home-court advantage. Maybe we just need a home court advantage vector in addition to the skill vector.


```{r compile and fit Stan Poisson Home Vector model, include = FALSE}
stan_model_poisson_hv_comp <- stan_model(file = "Stan_models/NBA_model_poisson_home_vector.stan")

stan_fit_poisson_hv <- sampling(stan_model_poisson_hv_comp, data = stan_data, iter = n_sims, 
                                # specifying initial values because they get rejected otherwise
                                init = list(list(home_court_advantage = rep(0, N_teams)),
                                            list(home_court_advantage = rep(1, N_teams)),
                                            list(home_court_advantage = rep(2, N_teams)),
                                            list(home_court_advantage = rep(3, N_teams))))
```


```{r analyze Stan Poisson Home Vector model}
print(stan_fit_poisson_hv, pars = c("team_skill", "home_court_advantage"))
```

We see different teams have different home court advantage. 


```{r posterior predictive check Poisson Home Vector model}
y_rep_poisson_hv <- as.matrix(stan_fit_poisson_hv, pars = c("away_score_rep", "home_score_rep", "score_difference_rep"))

ppc_dens_overlay(y = game_data$pts_away, yrep = y_rep_poisson_hv[1:400, 1:N_games])
ppc_dens_overlay(y = game_data$pts_home, yrep = y_rep_poisson_hv[1:400, (N_games + 1):(2 * N_games)])
ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_poisson_hv[1:400, (2 * N_games + 1):(3 * N_games)])
```



### Time-series structure

So far we have only fit the model on the entire season at once. Another approach would be to allow the skill of the teams to vary over time. We'll revisit the simpler normal model for score differences with home court, but now we'll update our estimate of the teams' skill parameters after each game.


```{r set up data for Stan time series model}
stan_data_ts <- list(
                    N_games = N_games,
                    N_teams = length(team_mapping_table$team_id),
                    away_points = game_data$pts_away,
                    home_points = game_data$pts_home,
                    overtime = game_data$overtime,
                    away_team_id = game_data$away_team_id,
                    home_team_id = game_data$home_team_id,
                    N_games_per_team = N_games_per_team
                  )

n_sims <- 10000
```


```{r compile and fit Stan time-series model, include = FALSE}
stan_model_time_comp <- stan_model(file = "Stan_models/NBA_model_timeseries.stan")

stan_fit_time <- sampling(stan_model_time_comp, data = stan_data_ts, iter = n_sims,
                          control = list(max_treedepth = 15))
```


```{r analyze Stan time-series model}
print(stan_fit_time, pars = c("home_court_advantage", "score_scale", "init_scale", "update_scale"))

check_hmc_diagnostics(stan_fit_time)
```

```{r further analyze Stan time-series model}
# options(max.print = 99999)
print(stan_fit_time, pars = "team_skill")
# options(max.print = 1000)
```

```{r posterior predictive check time-series model}
y_rep_time <- as.matrix(stan_fit_time, pars = "score_difference_rep")

ppc_dens_overlay(y = game_data$score_diff, yrep = y_rep_time[1:400, ])
```

The number of effective samples for update_scale is relatively low, but other than that the fit seems to be quite good. Even the team skill parameters (as far as we can see)

Now let's see how team skills evolve over time.


```{r extract skill over time}
team_skill <- extract(stan_fit_time, pars = "team_skill", permuted = TRUE)
team_skill <- team_skill[[1]]
mean_skill <- apply(team_skill, c(2, 3), mean) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
median_skill <- apply(team_skill, c(2, 3), median) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
upper_skill <- apply(team_skill, c(2, 3), quantile, probs = c(.96)) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
lower_skill <- apply(team_skill, c(2, 3), quantile, probs = c(.04)) %>% as.tibble() %>% mutate(team_name = team_mapping_table$team_name)
```

```{r analyze skill over time}
mean_skill <- mean_skill %>% gather(key = "game", value = "skill", -team_name) %>%
  mutate(game_number = as.numeric(substring(game, first = 2)))

mean_skill %>% group_by(team_name) %>% 
  summarize(mean_skill = mean(skill),
            median_skill = median(skill),
            max_skill = max(skill),
            min_skill = min(skill)) %>%
  arrange(desc(mean_skill)) %>% print(n = 30, digits = 2) -> ranked_teams

mean_skill %>% filter(team_name %in% ranked_teams$team_name[1:6]) %>% 
  ggplot(mapping = aes(x = game_number, y = skill, color = team_name)) +
  geom_line() + scale_color_brewer(type = "qual", palette = 3) +
  ggtitle("Top Teams in 2016-2017")

mean_skill %>% filter(team_name %in% ranked_teams$team_name[25:30]) %>% 
  ggplot(mapping = aes(x = game_number, y = skill, color = team_name)) +
  geom_line() + scale_color_brewer(type = "qual", palette = 3) +
  ggtitle("Bottom Teams in 2016-2017")


```



# Predicting new games


So far we have checked the model by seeing how well it re-creates the original data through simulations. Another way to check is to fit the model on part of the data and see if you can predict the outcomes of other games. Because of the time-series nature of the data we will only fit the model on sequential data and then predict on newer data.




```{r additional data manipulation for predictions}
#game_data_pred <- lookup_amount_games_played(game_data, team_mapping_table, N_games_per_team)



```
